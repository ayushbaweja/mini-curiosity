{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Curiosity Module (ICM) + A2C on FrozenLake-v1\n",
    "\n",
    "This notebook details the code implemented in `src/curiosity_a2c/*`. It explains how the code implement Pathak et al. (2017) on FrozenLake-v1.\n",
    "\n",
    "Curiosity-driven exploration gives agents a dense learning signal when extrinsic rewards vanish. Following Pathak et al. (2017), we pair the Intrinsic Curiosity Module with Advantage Actor-Critic (A2C) on the slippery 8x8 FrozenLake map to show how prediction errors in feature space reshape exploration for the exact code that lives under `src/curiosity_a2c`.\n",
    "\n",
    "We walk through three arcs that mirror the layout of the package:\n",
    "1. Rebuild the FrozenLake environment and helper utilities (`envs.py`, `wrappers.py`, `utils.py`).\n",
    "2. Train the baseline A2C agent with `train_baseline_a2c` to expose the exploration bottleneck under sparse rewards.\n",
    "3. Drop `ICMModule` + `ICMCallback` into the rollout loop (see `icm_module.py` and `icm_a2c.py`) so the agent optimizes Equations (1)-(7) from Pathak et al., then compare both policies side by side using `compare_models` and `record_episodes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "The next code cell mirrors what the scripts under `src/curiosity_a2c` do:\n",
    "- `train_baseline_a2c` from `baseline_a2c.py`\n",
    "- `train_a2c_with_icm` from `icm_a2c.py`\n",
    "- `compare_models` from `compare.py`\n",
    "- `test_model` from `utils.py`\n",
    "- `record_episodes` from `record_videos.py`\n",
    "\n",
    "We also import Gymnasium, Stable-Baselines3, Torch, and plotting utilities so the notebook runs the same code paths as the CLI scripts.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d90106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project dependencies (optional)\n",
    "# Run this only if your environment is missing packages; it installs the local project and its extras.\n",
    "import sys, subprocess\n",
    "\n",
    "cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".[dev]\"]\n",
    "print(\" \".join(cmd))\n",
    "subprocess.run(cmd, check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Set style for plots\u001b[39;00m\n\u001b[32m     19\u001b[39m sns.set_style(\u001b[33m'\u001b[39m\u001b[33mdarkgrid\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from curiosity_a2c.envs import make_frozenlake_env\n",
    "from curiosity_a2c.utils import make_env, test_model\n",
    "from curiosity_a2c.icm_module import ICMModule, ICMCallback\n",
    "from curiosity_a2c.baseline_a2c import train_baseline_a2c\n",
    "from curiosity_a2c.icm_a2c import train_a2c_with_icm\n",
    "from curiosity_a2c.compare import compare_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "`curiosity_a2c.envs.make_frozenlake_env` is the single entry point every script imports. It always builds `gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True, render_mode=\"rgb_array\")`, wraps it with `FrozenLakePixelWrapper` (see `wrappers.py`) to resize frames to 42x42 RGB tensors (CHW layout), and then optionally attaches a `Monitor` for episode stats/log files. The helper ignores the `render_mode` kwarg because the wrapper grabs frames directly, so any rendering (RGB array vs. human) is handled later when we build `DummyVecEnv` factories.\n",
    "\n",
    "`utils.make_env` is a thin alias used throughout `baseline_a2c.py`, `icm_a2c.py`, `record_videos.py`, and `utils.test_model`. When we spin up vectorized environments in the notebook we call the exact same factory, which means wrappers, observation shapes, and seeding logic always match the scripts. Use this section to instantiate env factories, seeds, log directories, and to demonstrate how to switch between monitor vs. non-monitor runs-everything else in the notebook reuses those handles.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the packaged FrozenLake env and inspect spaces\n",
    "env = make_env()\n",
    "obs_space, action_space = env.observation_space, env.action_space\n",
    "\n",
    "obs, _ = env.reset()\n",
    "print(\"Observation space:\", obs_space)\n",
    "print(\"Action space:\", action_space)\n",
    "print(\"Initial observation shape:\", obs.shape)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Curiosity-Driven Exploration (Pathak et al., 2017)\n",
    "\n",
    "`src/curiosity_a2c/icm_module.py` implements the Intrinsic Curiosity Module verbatim from the paper. `ICMModule` exposes the feature encoder, inverse model, forward model, and intrinsic reward that the rest of the package imports. The notebook recap below ties those code paths directly to the numbered equations so you know which part of the module to inspect.\n",
    "\n",
    "The policy objective from Equation (1) stays untouched:\n",
    "$$\n",
    "\\max_{\\theta_P} \\mathbb{E}_{\\pi(\\cdot;\\theta_P)} \\Bigg[\\sum_t r_t\\Bigg], \\tag{1}\n",
    "$$\n",
    "with $r_t = r_t^e + r_t^i$ as implemented when `ICMCallback` augments rewards inside `icm_module.py`.\n",
    "\n",
    "**Inverse dynamics encoder (Equations 2-3).** `ICMModule.inverse_model` predicts the action label exactly as\n",
    "$$\n",
    "\\hat{a}_t = g(s_t, s_{t+1}; \\theta_I), \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\min_{\\theta_I} \\mathcal{L}_I(\\hat{a}_t, a_t). \\tag{3}\n",
    "$$\n",
    "Those correspond to the cross-entropy loss computed in `ICMModule.forward` whenever the callback processes rollout buffers.\n",
    "\n",
    "**Forward dynamics model (Equations 4-5).** `ICMModule.forward_model` predicts the next feature vector instead of pixels:\n",
    "$$\n",
    "\\hat{\\phi}(s_{t+1}) = f(\\phi(s_t), a_t; \\theta_F), \\tag{4}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}_F = \\tfrac{1}{2} \\lVert \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\rVert_2^2. \\tag{5}\n",
    "$$\n",
    "\n",
    "**Intrinsic reward (Equation 6).** `ICMCallback` grabs the prediction error returned by `ICMModule` and adds it to the reward tensor before A2C updates:\n",
    "$$\n",
    " r_t^i = \\tfrac{\\eta}{2} \\lVert \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\rVert_2^2. \\tag{6}\n",
    "$$\n",
    "\n",
    "**Joint optimization (Equation 7).** The callback stitches everything together while `train_a2c_with_icm` injects the callback during `model.learn`:\n",
    "$$\n",
    "\\min_{\\theta_P,\\theta_I,\\theta_F} -\\lambda\\, \\mathbb{E}_\\pi \\Big[\\sum_t r_t\\Big] + (1-\\beta)\\mathcal{L}_I + \\beta \\mathcal{L}_F. \\tag{7}\n",
    "$$\n",
    "Paper defaults (`beta = 0.2`, `lambda = 0.1`, `eta = 0.01`) are surfaced as keyword arguments on `train_a2c_with_icm` so you can tweak them in code or here in the notebook.\n",
    "\n",
    "----------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mICMModule\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Intrinsic Curiosity Module with forward and inverse dynamics models.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33;03m    - Vector observations (MLP encoder) - used for FrozenLake\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation_space, action_space, feature_dim=\u001b[32m288\u001b[39m, beta=\u001b[32m0.2\u001b[39m, eta=\u001b[32m0.01\u001b[39m):\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the Intrinsic Curiosity Module using the env spaces\n",
    "icm_module = ICMModule(\n",
    "    observation_space=obs_space,\n",
    "    action_space=action_space,\n",
    "    beta=0.2,\n",
    "    eta=0.01,\n",
    ")\n",
    "icm_module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ICM Callback for Training\n",
    "\n",
    "`ICMCallback` (defined in `icm_module.py`) is the exact class the training script hands to SB3. Each rollout:\n",
    "1. Pulls `(s_t, a_t, s_{t+1})` from the `A2C` rollout buffer.\n",
    "2. Feeds them through `ICMModule.forward` to compute `L_I`, `L_F`, and `intrinsic_reward`.\n",
    "3. Forms `icm_loss = (1-beta) * L_I + beta * L_F` (same math that lives in the callback).\n",
    "4. Adds `r_t^i` from Equation (6) to the vectorized `VecNormalize` reward stream before the policy update.\n",
    "\n",
    "Because this is the same class imported by both the CLI (`icm_a2c.py`) and the notebook, any edits to `ICMCallback` immediately show up here.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wire up the ICM optimizer and callback (the same objects used during training)\n",
    "icm_optimizer = Adam(icm_module.parameters(), lr=1e-3)\n",
    "icm_callback = ICMCallback(icm_module, icm_optimizer, lambda_weight=0.1)\n",
    "icm_callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline A2C Training\n",
    "\n",
    "`train_baseline_a2c` (from `baseline_a2c.py`) documents the vanilla A2C stack that lives in the repo:\n",
    "- Creates `DummyVecEnv([make_env] * n_envs)` for training and another single-env copy for evaluation.\n",
    "- Wraps them with `VecNormalize`, setting `norm_obs=False` and `norm_reward=True` on the training side so only rewards are normalized (mirrors the script).\n",
    "- Registers `EvalCallback` (10 evaluation episodes every 5k steps) and `CheckpointCallback` (save every 10k steps) that write into `logs/<save_path>/`.\n",
    "- Calls `A2C(\"CnnPolicy\", ...)` with `use_rms_prop=True`, `normalize_advantage=True`, and the default hyperparameters from the script.\n",
    "\n",
    "The code cell under this heading calls the function directly, so its keyword arguments (learning rate, entropy coefficient, etc.) stay aligned with the Python source.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline A2C training configuration shared with the script\n",
    "BASELINE_CONFIG = dict(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    ent_coef=0.01,\n",
    ")\n",
    "BASELINE_CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A2C with ICM Training\n",
    "\n",
    "`train_a2c_with_icm` (from `icm_a2c.py`) mirrors the baseline but wires in the intrinsic curiosity machinery:\n",
    "- Builds the same `DummyVecEnv`/`VecNormalize` pair.\n",
    "- Instantiates `ICMModule` with the env observation and action spaces (so the encoder automatically picks the Conv net path) and places it on the SB3 policy device.\n",
    "- Sets up `ICMCallback`, passing in an `Adam` optimizer, `lambda_weight`, and the curiosity scalars (`icm_beta`, `icm_eta`). The callback mutates the rollout buffer rewards exactly like the script.\n",
    "- Appends `[icm_callback, eval_callback, checkpoint_callback]` to `model.learn` and, after training, saves `*_final.zip`, the ICM weights (`*_icm.pth`), and the VecNormalize stats (`*_vecnormalize.pkl`).\n",
    "\n",
    "Because the notebook calls the actual function, any change you make inside `icm_a2c.py` (e.g., toggling beta defaults or optimizer settings) automatically propagates here.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C + ICM training configuration (mirrors icm_a2c.py defaults)\n",
    "ICM_CONFIG = dict(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    ent_coef=0.01,\n",
    "    icm_lr=1e-3,\n",
    "    icm_beta=0.2,\n",
    "    icm_eta=0.01,\n",
    ")\n",
    "ICM_CONFIG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Comparison Utilities\n",
    "\n",
    "Everything under this heading is just re-exporting the helpers from `src/curiosity_a2c`:\n",
    "- `utils.test_model` loads `.zip` checkpoints with `A2C.load`, restores `VecNormalize` stats if present, and reports reward/length metrics.\n",
    "- `compare.compare_models` wraps two `test_model` calls, prints side-by-side statistics, and runs a SciPy t-test (same logic as `python -m curiosity_a2c.compare`).\n",
    "- `record_videos.record_episodes` / `record_single_episode` reuse the `make_env` factory plus Gymnasium's `RecordVideo` wrapper to log MP4s.\n",
    "\n",
    "The notebook cells call these functions without modification, so the documentation here reflects the true CLI behavior.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities come from curiosity_a2c.utils and curiosity_a2c.compare (imported above).\n",
    "# Plot helper remains local to the notebook.\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"Plot comparison between baseline and ICM models.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].boxplot(\n",
    "        [results['baseline']['rewards'], results['icm']['rewards']],\n",
    "        labels=['Baseline A2C', 'A2C + ICM'],\n",
    "    )\n",
    "    axes[0].set_ylabel('Episode Reward')\n",
    "    axes[0].set_title('Reward Distribution Comparison')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].boxplot(\n",
    "        [results['baseline']['lengths'], results['icm']['lengths']],\n",
    "        labels=['Baseline A2C', 'A2C + ICM'],\n",
    "    )\n",
    "    axes[1].set_ylabel('Episode Length (steps)')\n",
    "    axes[1].set_title('Episode Length Comparison')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models\n",
    "\n",
    "The training cells simply dispatch to `train_baseline_a2c(...)` and `train_a2c_with_icm(...)`. Artifacts land in `models/<run>` and `logs/<run>` exactly as in the scripts, and the returned SB3 objects are identical to what the CLI would produce. Adjust arguments here (timesteps, seed, reward normalization) the same way you would when editing the Python functions.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline A2C\n",
    "baseline_model, baseline_env = train_baseline_a2c(**BASELINE_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C with ICM\n",
    "icm_model, icm_module, icm_env = train_a2c_with_icm(**ICM_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Results\n",
    "\n",
    "This section calls `compare_models(baseline_path, icm_path, n_episodes)` from `compare.py`. Paths default to `models/baseline/a2c_frozenlake_baseline_final` and `models/icm/a2c_frozenlake_icm_final`, which match the save locations used in both training scripts. The printed summary (means, stds, success rates, t-test) is identical to running `python -m curiosity_a2c.compare`.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models\n",
    "comparison_results = compare_models(\n",
    "    \"models/baseline/a2c_frozenlake_baseline_final\",\n",
    "    \"models/icm/a2c_frozenlake_icm_final\",\n",
    "    n_episodes=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plot_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Individual Models\n",
    "\n",
    "We import `test_model` from `utils.py` and call it directly for whichever checkpoint you want to inspect. Because this is the same helper used by `compare_models` and the CLI utilities, the evaluation logic (deterministic actions, optional rendering, VecNormalize restoration) stays consistent across notebook and scripts.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test baseline model\n",
    "baseline_rewards, baseline_lengths = test_model(\n",
    "    \"models/baseline/a2c_frozenlake_baseline_final\",\n",
    "    n_episodes=10,\n",
    "    model_type='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ICM model\n",
    "icm_rewards, icm_lengths = test_model(\n",
    "    \"models/icm/a2c_frozenlake_icm_final\",\n",
    "    n_episodes=10,\n",
    "    model_type='icm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Training with TensorBoard\n",
    "\n",
    "Both training functions log to `./logs/<save_path>/tensorboard/` via the `tensorboard_log` argument passed to SB3. Launch TensorBoard pointing at `logs/` (either with the provided cell or `tensorboard --logdir logs` from the shell) to see policy losses, value losses, curiosity losses reported by `ICMCallback`, and evaluation metrics. No extra instrumentation is added in the notebook-the logs here are the exact ones produced by the scripts under `src/`.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Hyperparameters\n",
    "\n",
    "These knobs surface directly on the functions we call:\n",
    "\n",
    "**A2C knobs (`train_baseline_a2c`).** `total_timesteps`, `n_envs`, `learning_rate`, `n_steps`, `gamma`, `gae_lambda`, `ent_coef`, `vf_coef`, `max_grad_norm`, plus the checkpoint/log paths. Changing them here is equivalent to passing new values into the Python function.\n",
    "\n",
    "**ICM knobs (`train_a2c_with_icm`).**\n",
    "- `icm_lr`: learning rate for the `Adam` optimizer inside `icm_a2c.py`.\n",
    "- `icm_beta`: weighs forward vs. inverse loss inside `ICMModule` (Equation 7).\n",
    "- `icm_eta`: scales the intrinsic reward produced by `ICMModule`.\n",
    "- `lambda_weight`: multiplier passed to `ICMCallback` (placeholder similar to the paper's lambda term).\n",
    "\n",
    "**Reward composition + logging.** Reward normalization and checkpoint directories come straight from the Python scripts: `VecNormalize` makes intrinsic and extrinsic returns more stable, and everything is saved under `models/` + `logs/`. Keep `icm_eta <= 0.01` if you want intrinsic rewards to stay comparable to FrozenLake's extrinsic {0, 1} signal.\n",
    "\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The notebook now mirrors the live code in `src/curiosity_a2c`: baseline training (`baseline_a2c.py`), curiosity training (`icm_a2c.py` + `icm_module.py`), evaluation (`utils.py`, `compare.py`), and visualization (`record_videos.py`, TensorBoard logs) all run through the exact same functions. Treat this document as the guided tour of those modules-every heading corresponds to a callable you can also access via `python -m curiosity_a2c.<module>`.\n",
    "\n",
    "----------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-curiosity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
