{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Curiosity Module (ICM) with A2C on FrozenLake-v1\n",
    "\n",
    "Implementation of the Intrinsic Curiosity Module from Pathak et al. (2017) paper \"Curiosity-driven Exploration by Self-supervised Prediction\"\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Baseline A2C implementation on FrozenLake 8x8\n",
    "- A2C with ICM for curiosity-driven exploration\n",
    "- Comparison between both approaches\n",
    "\n",
    "**FrozenLake-v1** is a challenging discrete environment where the agent must navigate a slippery frozen lake to reach a goal while avoiding holes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "FrozenLake-v1 is a grid-world environment where:\n",
    "- **S**: Starting position\n",
    "- **F**: Frozen surface (safe to walk on)\n",
    "- **H**: Hole (fall to your doom)\n",
    "- **G**: Goal (where you want to reach)\n",
    "\n",
    "The 8x8 map is more challenging than the default 4x4, and `is_slippery=True` means actions have stochastic outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frozenlake_env(map_name=\"8x8\", is_slippery=True, render_mode=None):\n",
    "    \"\"\"Create a FrozenLake environment wrapped for neural-network policies.\"\"\"\n",
    "    env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        map_name=map_name,\n",
    "        is_slippery=is_slippery,\n",
    "        render_mode=render_mode,\n",
    "    )\n",
    "    # FlattenObservation converts the discrete state to a one-hot vector\n",
    "    env = FlattenObservation(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    \"\"\"Create the default FrozenLake environment.\"\"\"\n",
    "    return make_frozenlake_env(map_name=\"8x8\", is_slippery=True)\n",
    "\n",
    "\n",
    "# Test environment\n",
    "test_env = make_env()\n",
    "print(f\"Observation space: {test_env.observation_space}\")\n",
    "print(f\"Action space: {test_env.action_space}\")\n",
    "print(f\"Observation shape: {test_env.observation_space.shape}\")\n",
    "print(f\"Number of actions: {test_env.action_space.n}\")\n",
    "print(f\"\\nActions: 0=Left, 1=Down, 2=Right, 3=Up\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intrinsic Curiosity Module (ICM)\n",
    "\n",
    "**Paper Reference: Pathak et al. (2017), Section 2.2 and Figure 2**\n",
    "\n",
    "ICM consists of three components that work together to generate curiosity-driven intrinsic rewards:\n",
    "\n",
    "### Feature Encoder: φ(s)\n",
    "Encodes raw observations into a learned feature space that filters out task-irrelevant information (e.g., moving leaves, TV static).\n",
    "\n",
    "### Inverse Dynamics Model (Equations 2-3)\n",
    "Predicts action from state transitions:\n",
    "- **Equation 2**: `â_t = g(s_t, s_{t+1}; θ_I)`\n",
    "- **Equation 3**: `min_θI L_I(â_t, a_t)`\n",
    "\n",
    "The inverse model learns features φ(s) that encode **only** information relevant for predicting actions, filtering out uncontrollable aspects of the environment.\n",
    "\n",
    "### Forward Dynamics Model (Equations 4-5)\n",
    "Predicts next state features from current state and action:\n",
    "- **Equation 4**: `φ̂(s_{t+1}) = f(φ(s_t), a_t; θ_F)`\n",
    "- **Equation 5**: `L_F = (1/2) ||φ̂(s_{t+1}) - φ(s_{t+1})||²_2`\n",
    "\n",
    "### Intrinsic Reward (Equation 6) - **THE CURIOSITY SIGNAL**\n",
    "The prediction error in feature space serves as the curiosity reward:\n",
    "- **Equation 6**: `r^i_t = (η/2) ||φ̂(s_{t+1}) - φ(s_{t+1})||²_2`\n",
    "\n",
    "where η = 0.01 (default). High prediction error → novel/surprising state → high intrinsic reward → encourages exploration.\n",
    "\n",
    "### Overall Optimization (Equation 7)\n",
    "The complete system is trained end-to-end:\n",
    "\n",
    "**`min_{θP,θI,θF} [-λ E_π[Σ_t r_t] + (1-β)L_I + βL_F]`**\n",
    "\n",
    "where:\n",
    "- **β = 0.2** (paper default): weights forward vs inverse loss\n",
    "- **λ = 0.1** (paper default): weights policy gradient vs ICM learning  \n",
    "- **r_t = r^e_t + r^i_t**: total reward = extrinsic + intrinsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Intrinsic Curiosity Module with forward and inverse dynamics models.\n",
    "    \n",
    "    Supports both:\n",
    "    - Visual observations (Conv2D encoder)\n",
    "    - Vector observations (MLP encoder) - used for FrozenLake\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, feature_dim=288, beta=0.2, eta=0.01):\n",
    "        \"\"\"\n",
    "        Initialize ICM module.\n",
    "        \n",
    "        Args:\n",
    "            observation_space: Gym observation space\n",
    "            action_space: Gym action space\n",
    "            feature_dim: Dimension of learned feature representation\n",
    "            beta: Weight for forward loss vs inverse loss (paper uses 0.2)\n",
    "            eta: Scaling factor for intrinsic reward (paper uses 0.01)\n",
    "        \"\"\"\n",
    "        super(ICMModule, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.beta = beta\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Determine if observations are images or vectors\n",
    "        if len(observation_space.shape) == 3:\n",
    "            # Image observations (C, H, W)\n",
    "            self.is_image = True\n",
    "            n_input_channels = observation_space.shape[0]\n",
    "        elif len(observation_space.shape) == 1:\n",
    "            # Vector observations (e.g., FrozenLake)\n",
    "            self.is_image = False\n",
    "            self.obs_dim = observation_space.shape[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported observation space shape: {observation_space.shape}\")\n",
    "        \n",
    "        # Determine action space\n",
    "        if hasattr(action_space, 'n'):\n",
    "            self.action_dim = action_space.n\n",
    "            self.discrete = True\n",
    "        else:\n",
    "            self.action_dim = action_space.shape[0]\n",
    "            self.discrete = False\n",
    "        \n",
    "        # Create appropriate feature encoder\n",
    "        if self.is_image:\n",
    "            # Convolutional encoder for images (as in paper)\n",
    "            self.feature_encoder = nn.Sequential(\n",
    "                nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "                nn.ELU(),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "            \n",
    "            # Calculate feature dimension after convolutions\n",
    "            with torch.no_grad():\n",
    "                sample = torch.zeros(1, *observation_space.shape)\n",
    "                n_flatten = self.feature_encoder(sample).shape[1]\n",
    "            \n",
    "            # Project to desired feature dimension\n",
    "            self.feature_projection = nn.Linear(n_flatten, feature_dim)\n",
    "        else:\n",
    "            # MLP encoder for vector observations (FrozenLake uses this)\n",
    "            self.feature_encoder = nn.Sequential(\n",
    "                nn.Linear(self.obs_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            self.feature_projection = nn.Linear(128, feature_dim)\n",
    "        \n",
    "        # Inverse model: φ(st), φ(st+1) → at\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.action_dim)\n",
    "        )\n",
    "        \n",
    "        # Forward model: φ(st), at → φ(st+1)\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(feature_dim + self.action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, feature_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, obs):\n",
    "        \"\"\"\n",
    "        Encode observation to feature space.\n",
    "        \n",
    "        Args:\n",
    "            obs: Observation tensor\n",
    "            \n",
    "        Returns:\n",
    "            Feature representation φ(obs)\n",
    "        \"\"\"\n",
    "        features = self.feature_encoder(obs)\n",
    "        features = self.feature_projection(features)\n",
    "        return features\n",
    "    \n",
    "    def forward(self, obs, next_obs, action):\n",
    "        \"\"\"\n",
    "        Compute ICM losses and intrinsic reward.\n",
    "        \n",
    "        Args:\n",
    "            obs: Current observation\n",
    "            next_obs: Next observation\n",
    "            action: Action taken\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (forward_loss, inverse_loss, intrinsic_reward)\n",
    "        \"\"\"\n",
    "        # Encode observations to feature space\n",
    "        phi_obs = self.encode(obs)\n",
    "        phi_next_obs = self.encode(next_obs)\n",
    "        \n",
    "        # Inverse model loss: predict action from state transition\n",
    "        phi_concat = torch.cat([phi_obs, phi_next_obs], dim=1)\n",
    "        pred_action = self.inverse_model(phi_concat)\n",
    "        \n",
    "        if self.discrete:\n",
    "            inverse_loss = F.cross_entropy(pred_action, action.long())\n",
    "        else:\n",
    "            inverse_loss = F.mse_loss(pred_action, action)\n",
    "        \n",
    "        # Forward model loss: predict next state features\n",
    "        if self.discrete:\n",
    "            action_one_hot = F.one_hot(action.long(), num_classes=self.action_dim).float()\n",
    "        else:\n",
    "            action_one_hot = action\n",
    "        \n",
    "        phi_action = torch.cat([phi_obs, action_one_hot], dim=1)\n",
    "        pred_phi_next = self.forward_model(phi_action)\n",
    "        \n",
    "        forward_loss = F.mse_loss(pred_phi_next, phi_next_obs.detach())\n",
    "        \n",
    "        # Intrinsic reward = prediction error in feature space\n",
    "        intrinsic_reward = self.eta / 2 * torch.norm(\n",
    "            pred_phi_next - phi_next_obs.detach(), \n",
    "            dim=1, \n",
    "            p=2\n",
    "        ) ** 2\n",
    "        \n",
    "        return forward_loss, inverse_loss, intrinsic_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ICM Callback for Training\n",
    "\n",
    "**Paper Reference: Equation 7 - Joint optimization of policy and ICM**\n",
    "\n",
    "This callback integrates ICM training with A2C rollouts:\n",
    "\n",
    "1. **Collects transitions** (s_t, a_t, s_{t+1}) from rollout buffer\n",
    "2. **Computes ICM losses**:\n",
    "   - Inverse loss L_I (Eq 3)\n",
    "   - Forward loss L_F (Eq 5)  \n",
    "   - Combined: `ICM_loss = (1-β)L_I + βL_F` with β=0.2\n",
    "3. **Computes intrinsic rewards** r^i_t (Eq 6) using forward model prediction error\n",
    "4. **Adds intrinsic rewards** to extrinsic rewards: `r_t = r^e_t + r^i_t`\n",
    "5. **Updates ICM parameters** θ_I and θ_F via backpropagation\n",
    "\n",
    "The A2C policy then trains on the augmented rewards to maximize curiosity-driven exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback to train ICM module during A2C rollouts and add intrinsic rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, icm_module, icm_optimizer, lambda_weight=0.1, verbose=0):\n",
    "        \"\"\"\n",
    "        Initialize ICM callback.\n",
    "        \n",
    "        Args:\n",
    "            icm_module: ICMModule instance\n",
    "            icm_optimizer: Optimizer for ICM\n",
    "            lambda_weight: Weight for ICM loss (not currently used)\n",
    "            verbose: Verbosity level\n",
    "        \"\"\"\n",
    "        super(ICMCallback, self).__init__(verbose)\n",
    "        self.icm_module = icm_module\n",
    "        self.icm_optimizer = icm_optimizer\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.intrinsic_rewards = []\n",
    "        self.forward_losses = []\n",
    "        self.inverse_losses = []\n",
    "        self.icm_losses = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"Train ICM on collected rollout data and add intrinsic rewards.\"\"\"\n",
    "        rollout_buffer = self.model.rollout_buffer\n",
    "        \n",
    "        # Collect all transitions\n",
    "        obs_list = []\n",
    "        next_obs_list = []\n",
    "        actions_list = []\n",
    "        \n",
    "        buffer_size = rollout_buffer.observations.shape[0]\n",
    "        n_envs = rollout_buffer.observations.shape[1]\n",
    "        \n",
    "        # Extract transitions step by step\n",
    "        for step in range(buffer_size - 1):\n",
    "            for env in range(n_envs):\n",
    "                obs_list.append(rollout_buffer.observations[step, env])\n",
    "                next_obs_list.append(rollout_buffer.observations[step + 1, env])\n",
    "                actions_list.append(rollout_buffer.actions[step, env])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        obs = torch.FloatTensor(np.array(obs_list)).to(self.model.device)\n",
    "        next_obs = torch.FloatTensor(np.array(next_obs_list)).to(self.model.device)\n",
    "        actions = torch.FloatTensor(np.array(actions_list)).to(self.model.device)\n",
    "        \n",
    "        # Handle action shape for discrete actions (squeeze if needed)\n",
    "        if len(actions.shape) > 1 and actions.shape[-1] == 1:\n",
    "            actions = actions.squeeze(-1)\n",
    "        \n",
    "        # Train ICM\n",
    "        self.icm_optimizer.zero_grad()\n",
    "        forward_loss, inverse_loss, intrinsic_reward = self.icm_module(obs, next_obs, actions)\n",
    "        \n",
    "        # Combined ICM loss (Eq. 7 in paper)\n",
    "        icm_loss = (1 - self.icm_module.beta) * inverse_loss + self.icm_module.beta * forward_loss\n",
    "        icm_loss.backward()\n",
    "        self.icm_optimizer.step()\n",
    "        \n",
    "        # Add intrinsic rewards to rollout buffer\n",
    "        intrinsic_reward_np = intrinsic_reward.detach().cpu().numpy()\n",
    "        intrinsic_reward_reshaped = intrinsic_reward_np.reshape(buffer_size - 1, n_envs)\n",
    "        rollout_buffer.rewards[:-1] += intrinsic_reward_reshaped\n",
    "        \n",
    "        # Track statistics\n",
    "        self.intrinsic_rewards.extend(intrinsic_reward_np.tolist())\n",
    "        self.forward_losses.append(forward_loss.item())\n",
    "        self.inverse_losses.append(inverse_loss.item())\n",
    "        self.icm_losses.append(icm_loss.item())\n",
    "        \n",
    "        # LOG TO TENSORBOARD\n",
    "        if self.logger is not None:\n",
    "            # Log ICM-specific metrics\n",
    "            self.logger.record(\"icm/forward_loss\", forward_loss.item())\n",
    "            self.logger.record(\"icm/inverse_loss\", inverse_loss.item())\n",
    "            self.logger.record(\"icm/total_loss\", icm_loss.item())\n",
    "            self.logger.record(\"icm/mean_intrinsic_reward\", intrinsic_reward_np.mean())\n",
    "            self.logger.record(\"icm/std_intrinsic_reward\", intrinsic_reward_np.std())\n",
    "            self.logger.record(\"icm/max_intrinsic_reward\", intrinsic_reward_np.max())\n",
    "            self.logger.record(\"icm/min_intrinsic_reward\", intrinsic_reward_np.min())\n",
    "            \n",
    "            # Log cumulative statistics\n",
    "            if len(self.forward_losses) > 0:\n",
    "                self.logger.record(\"icm/avg_forward_loss\", np.mean(self.forward_losses[-100:]))\n",
    "                self.logger.record(\"icm/avg_inverse_loss\", np.mean(self.inverse_losses[-100:]))\n",
    "            \n",
    "            # Log reward composition\n",
    "            extrinsic_rewards = rollout_buffer.rewards[:-1] - intrinsic_reward_reshaped\n",
    "            self.logger.record(\"icm/mean_extrinsic_reward\", extrinsic_rewards.mean())\n",
    "            self.logger.record(\"icm/intrinsic_to_extrinsic_ratio\", \n",
    "                             intrinsic_reward_np.mean() / (abs(extrinsic_rewards.mean()) + 1e-8))\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(f\"ICM - Forward: {forward_loss.item():.4f}, \"\n",
    "                  f\"Inverse: {inverse_loss.item():.4f}, \"\n",
    "                  f\"Intrinsic Reward: {intrinsic_reward_np.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline A2C Training\n",
    "\n",
    "**Paper Reference: Equation 1 - Policy optimization**\n",
    "\n",
    "Trains a standard A2C agent that maximizes: `max_θP E_π[Σ_t r_t]`\n",
    "\n",
    "This baseline uses **only extrinsic rewards** r^e_t from the environment (no curiosity). In sparse reward environments like FrozenLake, the agent receives r^e_t = 0 until it reaches the goal, making exploration difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_a2c(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    n_steps=5,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    save_path=\"models/baseline/a2c_frozenlake_baseline\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train baseline A2C on FrozenLake-v1 (8x8)\n",
    "    \n",
    "    Args:\n",
    "        total_timesteps: Total training steps\n",
    "        n_envs: Number of parallel environments\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        n_steps: Number of steps to run for each environment per update\n",
    "        gamma: Discount factor\n",
    "        gae_lambda: Factor for GAE\n",
    "        ent_coef: Entropy coefficient for exploration\n",
    "        vf_coef: Value function coefficient\n",
    "        save_path: Path to save the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    env = DummyVecEnv([make_env for _ in range(n_envs)])\n",
    "    \n",
    "    # Normalize observations and rewards for better training\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "    \n",
    "    # Create evaluation environment\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    # Callbacks for evaluation and checkpointing\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"./logs/{save_path}/\",\n",
    "        log_path=f\"./logs/{save_path}/\",\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        n_eval_episodes=10\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=f\"./logs/{save_path}/checkpoints/\",\n",
    "        name_prefix=\"a2c_model\"\n",
    "    )\n",
    "    \n",
    "    # Create A2C model\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=gae_lambda,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=0.5,\n",
    "        use_rms_prop=True,\n",
    "        normalize_advantage=True,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./logs/{save_path}/tensorboard/\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Baseline A2C on FrozenLake-v1 (8x8)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total timesteps: {total_timesteps:,}\")\n",
    "    print(f\"Number of environments: {n_envs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Entropy coefficient: {ent_coef}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=[eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save the final model and normalization stats\n",
    "    model.save(f\"{save_path}_final\")\n",
    "    env.save(f\"{save_path}_vecnormalize.pkl\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"Model saved to: {save_path}_final.zip\")\n",
    "    print(f\"Normalization stats saved to: {save_path}_vecnormalize.pkl\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return model, env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A2C with ICM Training\n",
    "\n",
    "**Paper Reference: Equation 7 - Full ICM+A2C optimization**\n",
    "\n",
    "Trains A2C with curiosity-driven intrinsic rewards. The agent maximizes:\n",
    "\n",
    "`max_θP E_π[Σ_t (r^e_t + r^i_t)]`\n",
    "\n",
    "where r^i_t comes from the ICM forward model prediction error (Eq 6).\n",
    "\n",
    "**Key difference from baseline**: The intrinsic reward r^i_t provides a dense learning signal even when extrinsic rewards r^e_t are sparse or absent, enabling effective exploration of novel states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c_with_icm(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    n_steps=5,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=1.0,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.5,\n",
    "    icm_lr=1e-3,\n",
    "    icm_beta=0.2,\n",
    "    icm_eta=0.01,\n",
    "    lambda_weight=0.1,\n",
    "    save_path=\"models/icm/a2c_frozenlake_icm\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train A2C with ICM on FrozenLake-v1 (8x8)\n",
    "    \n",
    "    Args:\n",
    "        total_timesteps: Total training steps\n",
    "        n_envs: Number of parallel environments\n",
    "        learning_rate: Learning rate for A2C optimizer\n",
    "        n_steps: Number of steps per environment per update\n",
    "        gamma: Discount factor\n",
    "        gae_lambda: GAE lambda\n",
    "        ent_coef: Entropy coefficient\n",
    "        vf_coef: Value function coefficient\n",
    "        icm_lr: Learning rate for ICM\n",
    "        icm_beta: Weight for forward vs inverse loss (paper uses 0.2)\n",
    "        icm_eta: Scaling factor for intrinsic reward (paper uses 0.01)\n",
    "        lambda_weight: Weight for ICM loss in overall optimization\n",
    "        save_path: Path to save models\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, icm_module, env)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create vectorized environment\n",
    "    env = DummyVecEnv([make_env for _ in range(n_envs)])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "    \n",
    "    # Create evaluation environment\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "    eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "    \n",
    "    # Create A2C model\n",
    "    model = A2C(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=gae_lambda,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=0.5,\n",
    "        use_rms_prop=True,\n",
    "        normalize_advantage=True,\n",
    "        verbose=1,\n",
    "        tensorboard_log=f\"./logs/{save_path}/tensorboard/\"\n",
    "    )\n",
    "    \n",
    "    # Create ICM module\n",
    "    icm_module = ICMModule(\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        beta=icm_beta,\n",
    "        eta=icm_eta\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Create ICM optimizer\n",
    "    icm_optimizer = Adam(icm_module.parameters(), lr=icm_lr)\n",
    "    \n",
    "    # Setup callbacks\n",
    "    icm_callback = ICMCallback(\n",
    "        icm_module, \n",
    "        icm_optimizer, \n",
    "        lambda_weight=lambda_weight, \n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"./logs/{save_path}/\",\n",
    "        log_path=f\"./logs/{save_path}/\",\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        n_eval_episodes=10\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=f\"./logs/{save_path}/checkpoints/\",\n",
    "        name_prefix=\"a2c_icm_model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training A2C with ICM on FrozenLake-v1 (8x8)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total timesteps: {total_timesteps:,}\")\n",
    "    print(f\"Number of environments: {n_envs}\")\n",
    "    print(f\"A2C Learning rate: {learning_rate}\")\n",
    "    print(f\"ICM Learning rate: {icm_lr}\")\n",
    "    print(f\"ICM Beta (forward weight): {icm_beta}\")\n",
    "    print(f\"ICM Eta (reward scale): {icm_eta}\")\n",
    "    print(f\"Entropy coefficient: {ent_coef}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=[icm_callback, eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    model.save(f\"{save_path}_final\")\n",
    "    torch.save(icm_module.state_dict(), f\"{save_path}_icm.pth\")\n",
    "    env.save(f\"{save_path}_vecnormalize.pkl\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training completed!\")\n",
    "    print(f\"A2C Model saved to: {save_path}_final.zip\")\n",
    "    print(f\"ICM Module saved to: {save_path}_icm.pth\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return model, icm_module, env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing and Comparison Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, n_episodes=10, model_type='baseline'):\n",
    "    \"\"\"\n",
    "    Test a trained A2C model (works for both baseline and ICM versions)\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model (without .zip extension)\n",
    "        n_episodes: Number of episodes to test\n",
    "        model_type: 'baseline' or 'icm' for proper identification\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    model = A2C.load(model_path)\n",
    "    \n",
    "    # Create test environment\n",
    "    env = DummyVecEnv([make_env])\n",
    "    \n",
    "    # Load normalization stats if available\n",
    "    try:\n",
    "        vec_normalize_path = model_path.replace('_final', '') + '_vecnormalize.pkl'\n",
    "        env = VecNormalize.load(vec_normalize_path, env)\n",
    "        env.training = False\n",
    "        env.norm_reward = False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: Normalization stats not found, continuing without normalization\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {model_type.upper()} A2C Model\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward[0]\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {steps}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test Results ({n_episodes} episodes)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Average Reward: {np.mean(episode_rewards):.2f} (+/- {np.std(episode_rewards):.2f})\")\n",
    "    print(f\"Average Steps: {np.mean(episode_lengths):.2f} (+/- {np.std(episode_lengths):.2f})\")\n",
    "    print(f\"Success Rate: {sum(1 for r in episode_rewards if r > 0.0) / n_episodes * 100:.1f}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return episode_rewards, episode_lengths\n",
    "\n",
    "\n",
    "def compare_models(baseline_path, icm_path, n_episodes=20):\n",
    "    \"\"\"\n",
    "    Compare baseline A2C and ICM-enhanced A2C\n",
    "    \n",
    "    Args:\n",
    "        baseline_path: Path to baseline model (without .zip)\n",
    "        icm_path: Path to ICM model (without .zip)\n",
    "        n_episodes: Number of episodes for comparison\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARING BASELINE A2C vs A2C + ICM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n[1/2] Testing Baseline A2C...\")\n",
    "    baseline_rewards, baseline_lengths = test_model(\n",
    "        baseline_path, \n",
    "        n_episodes=n_episodes, \n",
    "        model_type='baseline'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[2/2] Testing A2C + ICM...\")\n",
    "    icm_rewards, icm_lengths = test_model(\n",
    "        icm_path, \n",
    "        n_episodes=n_episodes, \n",
    "        model_type='icm'\n",
    "    )\n",
    "    \n",
    "    # Comparison statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Metric':<30} {'Baseline A2C':<20} {'A2C + ICM':<20}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'Mean Reward':<30} {np.mean(baseline_rewards):<20.2f} {np.mean(icm_rewards):<20.2f}\")\n",
    "    print(f\"{'Std Reward':<30} {np.std(baseline_rewards):<20.2f} {np.std(icm_rewards):<20.2f}\")\n",
    "    print(f\"{'Mean Steps':<30} {np.mean(baseline_lengths):<20.2f} {np.mean(icm_lengths):<20.2f}\")\n",
    "    print(f\"{'Std Steps':<30} {np.std(baseline_lengths):<20.2f} {np.std(icm_lengths):<20.2f}\")\n",
    "    \n",
    "    baseline_success = sum(1 for r in baseline_rewards if r > 0.0) / n_episodes * 100\n",
    "    icm_success = sum(1 for r in icm_rewards if r > 0.0) / n_episodes * 100\n",
    "    print(f\"{'Success Rate (%)':<30} {baseline_success:<20.1f} {icm_success:<20.1f}\")\n",
    "    \n",
    "    # Statistical comparison\n",
    "    t_stat, p_value = stats.ttest_ind(baseline_rewards, icm_rewards)\n",
    "    print(f\"\\n{'Statistical Test (t-test)':<30}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        winner = \"A2C + ICM\" if np.mean(icm_rewards) > np.mean(baseline_rewards) else \"Baseline A2C\"\n",
    "        improvement = abs(np.mean(icm_rewards) - np.mean(baseline_rewards))\n",
    "        print(f\"  Result: {winner} is significantly better (p < 0.05)\")\n",
    "        print(f\"  Mean improvement: {improvement:.2f} reward points\")\n",
    "    else:\n",
    "        print(f\"  Result: No significant difference (p >= 0.05)\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'baseline': {\n",
    "            'rewards': baseline_rewards, \n",
    "            'lengths': baseline_lengths,\n",
    "            'mean_reward': np.mean(baseline_rewards),\n",
    "            'success_rate': baseline_success\n",
    "        },\n",
    "        'icm': {\n",
    "            'rewards': icm_rewards, \n",
    "            'lengths': icm_lengths,\n",
    "            'mean_reward': np.mean(icm_rewards),\n",
    "            'success_rate': icm_success\n",
    "        },\n",
    "        'statistics': {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"Plot comparison between baseline and ICM models\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Rewards comparison\n",
    "    axes[0].boxplot([results['baseline']['rewards'], results['icm']['rewards']], \n",
    "                    labels=['Baseline A2C', 'A2C + ICM'])\n",
    "    axes[0].set_ylabel('Episode Reward')\n",
    "    axes[0].set_title('Reward Distribution Comparison')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths comparison\n",
    "    axes[1].boxplot([results['baseline']['lengths'], results['icm']['lengths']], \n",
    "                    labels=['Baseline A2C', 'A2C + ICM'])\n",
    "    axes[1].set_ylabel('Episode Length (steps)')\n",
    "    axes[1].set_title('Episode Length Comparison')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models\n",
    "\n",
    "Run these cells to train both models. Set `total_timesteps` according to your needs (200,000 is recommended for FrozenLake 8x8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline A2C\n",
    "baseline_model, baseline_env = train_baseline_a2c(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    ent_coef=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C with ICM\n",
    "icm_model, icm_module, icm_env = train_a2c_with_icm(\n",
    "    total_timesteps=200_000,\n",
    "    n_envs=4,\n",
    "    learning_rate=7e-4,\n",
    "    ent_coef=0.01,\n",
    "    icm_lr=1e-3,\n",
    "    icm_beta=0.2,\n",
    "    icm_eta=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models\n",
    "comparison_results = compare_models(\n",
    "    \"models/baseline/a2c_frozenlake_baseline_final\",\n",
    "    \"models/icm/a2c_frozenlake_icm_final\",\n",
    "    n_episodes=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plot_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test baseline model\n",
    "baseline_rewards, baseline_lengths = test_model(\n",
    "    \"models/baseline/a2c_frozenlake_baseline_final\",\n",
    "    n_episodes=10,\n",
    "    model_type='baseline'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ICM model\n",
    "icm_rewards, icm_lengths = test_model(\n",
    "    \"models/icm/a2c_frozenlake_icm_final\",\n",
    "    n_episodes=10,\n",
    "    model_type='icm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Training with TensorBoard\n",
    "\n",
    "To view training metrics in your browser, run the command below (TensorBoard is a long-running process, so you may prefer to run it in a separate terminal):\n",
    "\n",
    "Then navigate to http://localhost:6006 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Hyperparameters\n",
    "\n",
    "### A2C Parameters\n",
    "- `learning_rate`: 7e-4 (default)\n",
    "- `n_steps`: 5 (rollout length)\n",
    "- `gamma`: 0.99 (discount factor)\n",
    "- `ent_coef`: 0.01 (entropy coefficient for exploration)\n",
    "\n",
    "### ICM Parameters\n",
    "- `icm_lr`: 1e-3 (ICM learning rate)\n",
    "- `icm_beta`: 0.2 (forward vs inverse loss weight, from paper)\n",
    "- `icm_eta`: 0.01 (intrinsic reward scaling, from paper)\n",
    "\n",
    "### Experiment with different values:\n",
    "- Higher `icm_beta` → More focus on forward model (feature prediction)\n",
    "- Higher `icm_eta` → Larger intrinsic rewards\n",
    "- Higher `ent_coef` → More exploration\n",
    "\n",
    "### FrozenLake-specific considerations:\n",
    "- Success rate is the most important metric (reaching the goal)\n",
    "- The environment is sparse reward (0 everywhere, 1 at goal)\n",
    "- ICM helps by providing intrinsic rewards for exploring novel states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "1. **Baseline A2C**: Standard advantage actor-critic algorithm on FrozenLake 8x8\n",
    "2. **ICM Module**: Curiosity-driven exploration using:\n",
    "   - Feature encoder φ(s) - converts one-hot states to learned features\n",
    "   - Inverse model (predicts action from state transitions)\n",
    "   - Forward model (predicts next state features)\n",
    "   - Intrinsic reward = forward model prediction error\n",
    "\n",
    "3. **Comparison**: Statistical comparison showing whether ICM improves performance\n",
    "\n",
    "The ICM helps the agent explore the frozen lake more effectively by providing intrinsic rewards for novel/surprising states, which is particularly valuable in sparse reward environments like FrozenLake where the agent only receives reward upon reaching the goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mini-curiosity",
   "language": "python",
   "name": "mini-curiosity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
